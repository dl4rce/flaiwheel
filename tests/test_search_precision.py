"""Search precision benchmark for Flaiwheel's DocsIndexer.

Indexes a curated corpus of realistic knowledge-base documents, then
measures P@1, P@5, and MRR against a gold-standard query set.

Thresholds are conservative baselines — they should improve once hybrid
(vector + BM25) search is tuned further.
"""
import pytest
from pathlib import Path
from flaiwheel.indexer import DocsIndexer


# ── Curated benchmark corpus ─────────────────────────────────────────

BENCHMARK_DOCS: dict[str, str] = {
    # --- Architecture (2) ---
    "architecture/auth-architecture.md": """\
# Authentication Architecture

## Overview

The platform uses JSON Web Tokens (JWT) for stateless authentication
across all microservices.  Tokens are signed with RS256 and contain
user-id, roles, and a tenant claim.

## Token Lifecycle

1. Client sends credentials to the Auth Service.
2. Auth Service validates against the user store and issues an access
   token (15 min) and a refresh token (7 days).
3. Every downstream service verifies the JWT signature using the
   public key fetched from the JWKS endpoint.

## Decisions

- RS256 over HS256 so services never need the signing secret.
- Short-lived access tokens reduce the blast radius of leaked tokens.
- Refresh tokens are stored server-side and revocable.

## Trade-offs

Stateless JWTs cannot be instantly revoked; we accept a 15-minute
window and mitigate with a token-deny-list for critical revocations.
""",
    "architecture/database-design.md": """\
# Database Design

## Overview

We use PostgreSQL 16 as the primary relational store and Redis 7 for
caching hot paths.  Each microservice owns its own schema.

## Schema Conventions

- Table names: `snake_case`, singular (e.g. `order`, `line_item`).
- Primary keys: UUIDv7 generated by the application layer.
- Timestamps: `created_at` and `updated_at` columns on every table,
  stored as `timestamptz`.

## Indexing Strategy

Partial indexes on `status` columns keep write overhead low while
speeding up the most common filtered queries.

## Data Isolation

Row-level security (RLS) policies enforce tenant isolation at the
database layer, so even a compromised service cannot read cross-tenant
data.
""",

    # --- API (2) ---
    "api/user-api.md": """\
# User API Endpoints

## POST /api/v1/users

Create a new user account.

Request body:
```json
{
  "email": "alice@example.com",
  "password": "s3cret!",
  "display_name": "Alice"
}
```

Response `201 Created`:
```json
{"id": "usr_abc123", "email": "alice@example.com"}
```

## GET /api/v1/users/:id

Retrieve a user profile.  Requires `Authorization: Bearer <token>`.

## DELETE /api/v1/users/:id

Soft-delete a user.  Sets `deleted_at` timestamp; data is purged after
30 days by the retention worker.
""",
    "api/payment-api.md": """\
# Payment API

## POST /api/v1/payments

Initiate a payment.  Calls the Stripe Charges API under the hood.

Request body:
```json
{
  "order_id": "ord_xyz",
  "amount_cents": 4999,
  "currency": "eur",
  "payment_method_id": "pm_card_visa"
}
```

Response `202 Accepted`:
```json
{"payment_id": "pay_001", "status": "processing"}
```

## GET /api/v1/payments/:id

Returns payment status.  Possible values: `processing`, `succeeded`,
`failed`, `refunded`.

## POST /api/v1/payments/:id/refund

Issue a full or partial refund.  Requires `amount_cents` in the body.
""",

    # --- Bugfix (2) ---
    "bugfix-log/fix-jwt-token-expiry.md": """\
# Fix: JWT Token Expiry Off-by-One

## Problem

Users were logged out exactly at the token's `exp` claim instead of
receiving a grace-period refresh.  The comparison used `>=` instead of
`>`, causing a one-second window where valid tokens were rejected.

## Root Cause

The middleware compared `current_time >= token.exp` but the spec says
a token is invalid only when `current_time > token.exp`.

## Solution

Changed the comparison operator from `>=` to `>` in
`auth/middleware.py:42` and added a 5-second clock-skew tolerance
(`leeway=5`).

## Lesson Learned

Always use the `leeway` parameter when verifying JWTs to account for
clock drift between services.
""",
    "bugfix-log/fix-race-condition-cache.md": """\
# Fix: Race Condition in Redis Cache Invalidation

## Problem

Under high concurrency, stale product prices were served for up to
60 seconds after an admin updated them.

## Root Cause

The cache-invalidation message was published before the database
transaction committed.  A concurrent reader would miss the DB update,
re-populate the cache with the old price, and the invalidation event
arrived too late.

## Solution

Moved the cache-invalidation publish call into a `transaction.on_commit`
callback so the event fires only after the row is visible to other
transactions.

```python
transaction.on_commit(lambda: redis.publish("invalidate", key))
```

## Lesson Learned

Never publish cache invalidation events outside the transaction boundary.
Use `on_commit` hooks to guarantee ordering.
""",

    # --- Best Practices (2) ---
    "best-practices/error-handling.md": """\
# Error Handling Patterns

## Principle

All services must return structured error responses using RFC 7807
Problem Details format.

## Response Format

```json
{
  "type": "https://errors.example.com/validation",
  "title": "Validation Error",
  "status": 422,
  "detail": "Field 'email' is not a valid address.",
  "instance": "/api/v1/users"
}
```

## Guidelines

- Never expose stack traces or internal paths in production responses.
- Log the full traceback server-side with a correlation ID.
- Map all unhandled exceptions to a generic 500 response.
- Use custom exception classes that carry an error `type` URI.
""",
    "best-practices/logging-standards.md": """\
# Logging Standards

## Format

All services emit structured JSON logs with at minimum these fields:
`timestamp`, `level`, `service`, `correlation_id`, `message`.

## Log Levels

| Level   | When to use                                      |
|---------|--------------------------------------------------|
| DEBUG   | Verbose detail for local development only        |
| INFO    | Normal operations: request handled, job started  |
| WARNING | Recoverable issues: retry succeeded, deprecated  |
| ERROR   | Failed operation that needs investigation        |

## Correlation IDs

Every inbound HTTP request generates a UUID correlation ID that is
propagated through all downstream calls.  This is mandatory for
distributed tracing.

## Sensitive Data

Never log passwords, tokens, credit card numbers, or PII.  Use a
scrubbing middleware to redact known sensitive field names.
""",

    # --- Setup (2) ---
    "setup/docker-deployment.md": """\
# Docker Deployment Guide

## Prerequisites

- Docker Engine 24+ and Docker Compose v2.
- A `.env` file with `DATABASE_URL`, `REDIS_URL`, and `JWT_SECRET`.

## Quick Start

```bash
docker compose up -d
docker compose exec app python manage.py migrate
docker compose exec app python manage.py createsuperuser
```

## Production Checklist

1. Use multi-stage builds to keep images under 150 MB.
2. Set `restart: unless-stopped` for all services.
3. Mount a named volume for PostgreSQL data.
4. Expose only port 443 via a reverse proxy; never expose the app port
   directly.

## Health Checks

The `docker-compose.yml` includes health checks that hit `/healthz` on
each service.  The orchestrator restarts containers that fail three
consecutive checks.
""",
    "setup/ci-cd-pipeline.md": """\
# CI/CD Pipeline Setup

## Overview

We use GitHub Actions for continuous integration and Argo CD for
continuous deployment to Kubernetes.

## CI Workflow (GitHub Actions)

1. **Lint** — `ruff check` and `mypy` on every push.
2. **Test** — `pytest -x --cov` must pass with >= 80% coverage.
3. **Build** — Docker image tagged with the short SHA.
4. **Push** — Image pushed to GitHub Container Registry.

## CD Workflow (Argo CD)

Argo CD watches the `deploy/` folder in the `main` branch.  When a new
image tag appears in `kustomization.yaml`, Argo syncs automatically.

## Secrets Management

All secrets are stored in GitHub Actions encrypted secrets and injected
as environment variables at build time.  Production secrets are managed
by Sealed Secrets in the cluster.
""",

    # --- Changelog (1) ---
    "changelog/release-v2.5.0.md": """\
# Release v2.5.0

**Date:** 2026-02-15

## New Features

- Added multi-tenant support with row-level security.
- Payment refund endpoint (`POST /payments/:id/refund`).
- Real-time webhook notifications for payment status changes.

## Improvements

- Reduced Docker image size from 320 MB to 140 MB via multi-stage build.
- JWT verification now uses a 5-second leeway for clock skew.

## Bug Fixes

- Fixed race condition in cache invalidation (see bugfix-log).
- Corrected off-by-one in token expiry check.

## Breaking Changes

- `/api/v1/users` response now includes `tenant_id` field.
- Removed deprecated `/api/v0/` endpoints.
""",

    # --- Test Cases (2) ---
    "tests/auth-flow-tests.md": """\
# Auth Flow Test Cases

## Test: Successful Login

**Scenario:** User logs in with correct credentials.
**Steps:**
1. POST `/api/v1/auth/login` with valid email and password.
2. Assert response status is `200`.
3. Assert body contains `access_token` and `refresh_token`.

## Test: Expired Token Refresh

**Scenario:** Access token has expired; client uses refresh token.
**Steps:**
1. Wait for access token to expire.
2. POST `/api/v1/auth/refresh` with the refresh token.
3. Assert new access token is returned with a fresh `exp` claim.

## Test: Invalid Credentials

**Scenario:** User provides a wrong password.
**Steps:**
1. POST `/api/v1/auth/login` with incorrect password.
2. Assert response status is `401`.
3. Assert body contains error type `authentication_failed`.
""",
    "tests/api-rate-limiting-tests.md": """\
# API Rate Limiting Test Cases

## Test: Under Limit

**Scenario:** Client sends requests below the rate limit threshold.
**Steps:**
1. Send 50 requests to `/api/v1/users` within 60 seconds.
2. Assert all responses return `200`.
3. Assert `X-RateLimit-Remaining` header decreases correctly.

## Test: Over Limit

**Scenario:** Client exceeds the rate limit.
**Steps:**
1. Send 120 requests to `/api/v1/users` within 60 seconds.
2. Assert responses after the 100th return `429 Too Many Requests`.
3. Assert `Retry-After` header is present with a positive value.

## Test: Rate Limit Reset

**Scenario:** Rate limit window resets after the configured period.
**Steps:**
1. Exhaust the rate limit.
2. Wait for the reset window (60 seconds).
3. Assert the next request returns `200`.
""",
}


# ── Gold-standard query → expected source ────────────────────────────

GOLD_STANDARD = [
    {
        "query": "JWT token authentication flow and signing",
        "expected_source": "architecture/auth-architecture.md",
    },
    {
        "query": "PostgreSQL database schema design conventions",
        "expected_source": "architecture/database-design.md",
    },
    {
        "query": "how to create a new user account via the API",
        "expected_source": "api/user-api.md",
    },
    {
        "query": "payment processing Stripe charges endpoint",
        "expected_source": "api/payment-api.md",
    },
    {
        "query": "JWT token expiry off by one bug fix",
        "expected_source": "bugfix-log/fix-jwt-token-expiry.md",
    },
    {
        "query": "race condition cache invalidation stale data",
        "expected_source": "bugfix-log/fix-race-condition-cache.md",
    },
    {
        "query": "structured error response RFC 7807 problem details",
        "expected_source": "best-practices/error-handling.md",
    },
    {
        "query": "structured JSON logging correlation ID standards",
        "expected_source": "best-practices/logging-standards.md",
    },
    {
        "query": "deploy application with Docker Compose containers",
        "expected_source": "setup/docker-deployment.md",
    },
    {
        "query": "GitHub Actions CI CD pipeline Argo CD Kubernetes",
        "expected_source": "setup/ci-cd-pipeline.md",
    },
    {
        "query": "release notes version 2.5 new features breaking changes",
        "expected_source": "changelog/release-v2.5.0.md",
    },
    {
        "query": "login authentication test cases refresh token scenario",
        "expected_source": "tests/auth-flow-tests.md",
    },
    {
        "query": "API rate limiting 429 too many requests test",
        "expected_source": "tests/api-rate-limiting-tests.md",
    },
]


# ── Metrics ──────────────────────────────────────────────────────────

def precision_at_k(results: list[dict], expected_source: str, k: int = 1) -> float:
    """1.0 if expected source is in top-k results, else 0.0."""
    for r in results[:k]:
        if r["source"].endswith(expected_source) or expected_source in r["source"]:
            return 1.0
    return 0.0


def mean_reciprocal_rank(results: list[dict], expected_source: str) -> float:
    """1/rank where expected source first appears, or 0."""
    for i, r in enumerate(results, 1):
        if r["source"].endswith(expected_source) or expected_source in r["source"]:
            return 1.0 / i
    return 0.0


# ── Test suite ───────────────────────────────────────────────────────

class TestSearchPrecision:

    @pytest.fixture(autouse=True)
    def setup_benchmark(self, config, tmp_docs):
        """Write the curated benchmark docs and build the index."""
        for rel_path, content in BENCHMARK_DOCS.items():
            doc_file = tmp_docs / rel_path
            doc_file.parent.mkdir(parents=True, exist_ok=True)
            doc_file.write_text(content)

        self.indexer = DocsIndexer(config)
        result = self.indexer.index_all(force=True)
        assert result["status"] == "success"
        assert result["chunks_total"] > 0

    def test_precision_at_1(self):
        """P@1: is the top result from the correct document?"""
        scores = []
        for case in GOLD_STANDARD:
            results = self.indexer.search(case["query"], top_k=5)
            p = precision_at_k(results, case["expected_source"], k=1)
            tag = "HIT" if p else "MISS"
            top_src = results[0]["source"] if results else "(empty)"
            print(f"  [{tag}] {case['query'][:50]:50s} → {top_src}")
            scores.append(p)

        p1 = sum(scores) / len(scores)
        print(f"\n  ── P@1 = {p1:.0%} ({sum(scores):.0f}/{len(scores)}) ──")
        assert p1 >= 0.5, f"P@1 too low: {p1:.0%}"

    def test_precision_at_5(self):
        """P@5: is the correct document anywhere in the top 5?"""
        scores = []
        for case in GOLD_STANDARD:
            results = self.indexer.search(case["query"], top_k=5)
            p = precision_at_k(results, case["expected_source"], k=5)
            tag = "HIT" if p else "MISS"
            sources = [r["source"] for r in results]
            print(f"  [{tag}] {case['query'][:50]:50s} → {sources}")
            scores.append(p)

        p5 = sum(scores) / len(scores)
        print(f"\n  ── P@5 = {p5:.0%} ({sum(scores):.0f}/{len(scores)}) ──")
        assert p5 >= 0.7, f"P@5 too low: {p5:.0%}"

    def test_mrr(self):
        """MRR: average reciprocal rank of the correct result."""
        scores = []
        for case in GOLD_STANDARD:
            results = self.indexer.search(case["query"], top_k=5)
            rr = mean_reciprocal_rank(results, case["expected_source"])
            print(f"  [RR={rr:.2f}] {case['query'][:50]:50s}")
            scores.append(rr)

        mrr = sum(scores) / len(scores)
        print(f"\n  ── MRR = {mrr:.2f} ──")
        assert mrr >= 0.5, f"MRR too low: {mrr:.2f}"

    def test_type_filter_precision(self):
        """Type-filtered search should return only matching types."""
        filter_cases = [
            ("authentication JWT signing", "architecture"),
            ("payment endpoint refund", "api"),
            ("cache invalidation bug", "bugfix"),
            ("logging correlation ID", "best-practice"),
            ("Docker Compose deployment", "setup"),
        ]
        all_correct = 0
        for query, doc_type in filter_cases:
            results = self.indexer.search(query, top_k=3, type_filter=doc_type)
            correct = all(r["type"] == doc_type for r in results)
            if correct:
                all_correct += 1
            print(f"  [{'OK' if correct else 'FAIL'}] type={doc_type:15s} query={query[:40]}")

        ratio = all_correct / len(filter_cases)
        print(f"\n  ── Type filter accuracy = {ratio:.0%} ({all_correct}/{len(filter_cases)}) ──")
        assert ratio == 1.0, f"Type filter returned wrong types in {len(filter_cases) - all_correct} cases"
